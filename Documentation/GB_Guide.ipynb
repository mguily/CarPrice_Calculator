{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9eb582ae-857c-4559-ba0e-9134296d11db",
   "metadata": {},
   "source": [
    "# Gradient Boosting Regressor\n",
    "\n",
    "El **Gradient Boosting Regressor** es un estimador (implementaci칩n del algoritmo) que construye un modelo de manera secuencial, optimizando una funci칩n de p칠rdida.  \n",
    "Su funcionamiento se basa en los siguientes pasos:\n",
    "\n",
    "1. Se entrena un modelo inicial (normalmente un 치rbol de decisi칩n) para predecir el valor objetivo.\n",
    "2. Se calcula el **error residual** entre las predicciones y los valores reales.\n",
    "3. Se entrena un nuevo 치rbol para predecir los errores residuales.\n",
    "4. Se suma la contribuci칩n del nuevo 치rbol al modelo existente, ponderado por una **tasa de aprendizaje**.\n",
    "5. Se repiten los pasos 2 a 4 hasta completar el n칰mero de iteraciones (el n칰mero de 치rboles se ajusta en `n_estimators`).\n",
    "\n",
    "Para conjuntos de datos grandes (`n_samples >= 10,000`), se recomienda usar `HistGradientBoostingRegressor`, una variante m치s eficiente de este algoritmo.\n",
    "\n",
    "## Par치metros principales\n",
    "\n",
    "El `GradientBoostingRegressor` ofrece diversos par치metros que ajustan el rendimiento del estimador, entre ellos se encuentran:\n",
    "\n",
    "- **`loss`**: Funci칩n de p칠rdida a optimizar.\n",
    "- **`learning_rate`**: Tasa de aprendizaje que controla cu치nto contribuye cada 치rbol al modelo final (por defecto, `0.1`).\n",
    "- **`n_estimators`**: N칰mero de 치rboles en el conjunto (por defecto, `100`).\n",
    "- **`subsample`**: Fracci칩n de datos usados para entrenar cada 치rbol. (por defecto, `1.0`).\n",
    "- **`criterion`**: Criterio de divisi칩n en los 치rboles (por defecto, `'friedman_mse'`).\n",
    "- **`min_samples_split`**: M칤nimo de muestras requerido para dividir un nodo (por defecto, `2`).\n",
    "- **`min_samples_leaf`**: M칤nimo de muestras que debe tener un nodo hoja (por defecto, `1`).\n",
    "- **`max_depth`**: Profundidad m치xima de los 치rboles (por defecto, `3`).\n",
    "- **`random_state`**: Controla la aleatoriedad del modelo, si no se fija, cada vez que entrenes el modelo obtendr치s resultados ligeramente diferentes.\n",
    "\n",
    "Estos par치metros permiten controlar el sobreajuste (Evitar que un modelo se adapte demasiado a los datos de entrenamiento).\n",
    "\n",
    "---\n",
    "\n",
    "## Interpretaci칩n del modelo en uso\n",
    "\n",
    "La siguiente configuraci칩n ha sido utilizada en el sistema:\n",
    "\n",
    "```python\n",
    "model = GradientBoostingRegressor(\n",
    "    n_estimators=1000, \n",
    "    learning_rate=0.01, \n",
    "    max_depth=6, \n",
    "    subsample=0.8, \n",
    "    random_state=42\n",
    ")\n",
    "```\n",
    "\n",
    "### Explicaci칩n de los par치metros utilizados:\n",
    "\n",
    "1. **`n_estimators=1000`**: Se entrenan **1,000 치rboles**, lo que permite corregir errores gradualmente y mejorar la precisi칩n del modelo.\n",
    "2. **`learning_rate=0.01`**: Cada nuevo 치rbol contribuye solo un **1%** al modelo final, evitando sobreajuste y mejorando la estabilidad.\n",
    "3. **`max_depth=6`**: Se permite que los 치rboles tengan **hasta 6 niveles de profundidad**, capturando patrones m치s complejos. Puede clasificar datos en 64 categor칤as diferentes antes de llegar a una decisi칩n final.\n",
    "4. **`subsample=0.8`**: Cada 치rbol se entrena con el **80% de los datos**, introduciendo aleatoriedad para mejorar la generalizaci칩n.\n",
    "5. **`random_state=42`**: Fija un n칰mero para que el entrenamiento siempre d칠 los mismos resultados.\n",
    "\n",
    "### Conclusi칩n sobre la configuraci칩n\n",
    "\n",
    "Esta configuraci칩n busca lograr un buen equilibrio entre precisi칩n y capacidad de generalizaci칩n (capacidad del modelo para realizar predicciones precisas sobre nuevos datos que no ha visto antes). Usar muchos 치rboles (1,000) con una tasa de aprendizaje baja (0.01) hace que el modelo mejore poco a poco, evitando errores grandes y haci칠ndolo m치s estable. El submuestreo (usar solo el 80% de los datos en cada 치rbol) introduce algo de aleatoriedad, lo que ayuda a que el modelo no se ajuste demasiado a los datos de entrenamiento y funcione mejor con nuevos datos. Sin embargo, entrenar tantos 치rboles con una profundidad de 6 puede hacer que el proceso sea m치s lento y requiera m치s recursos 丘멆잺.\n",
    "\n",
    "---\n",
    "\n",
    "## Fuente\n",
    "Parte del contenido de esta documentaci칩n se basa en la documentaci칩n oficial de Scikit-Learn.\n",
    "Puedes encontrar m치s informaci칩n en:  \n",
    "游댕 [Scikit-Learn - GradientBoostingRegressor](https://scikit-learn.org/dev/modules/generated/sklearn.ensemble.GradientBoostingRegressor.html#sklearn.ensemble.GradientBoostingRegressor)\n",
    "Todo el contenido original pertenece a Scikit-Learn."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
